# 已知问题

经过最终答辩现场的交流与后期查阅资料，学习到了很多新的内容，也发现了一些自己之前没有注意到的问题

> 模型显存利用率的问题

- Triplet Loss 的**数据**占用 3 倍显存，如果使用 Softmax 方法也许会更加简单些（两位学长均使用了 Softmax）
- Adam 优化器**模型**占用 4 倍显存，如果使用 SGD 将只占用 2 倍显存（但不清楚 SGD 在 BatchAll 下能否收敛），模型显存利用率高一些的话，就可以搭建更加深的网络

> 图片大小问题

- 我最终输入的图片 Size 已经非常小了（虽然我觉得肉眼大致能分辨出来），这也使得模型的大小有着一定的限制，但是如果使用更大的 Size 的话，我必须要重新优化内存以及显存的问题了……
- 如果将二值图作为三通道的图片，也许能够利用 Transfer Learning

> 模型的选用

如果显存优化的足够好，也许直接调用某些现成模型也是可以的，但现在……任何已有模型我的显存都是装不下的，只能运行我自己搭的各种 Mini 版模型（各种魔改）

> Batch Size 的问题

虽然说一般模型确实 Batch Size 太小不会有太多影响，但考虑到模型中大量使用了 **Batch Normaliztion**（不用的话 train 不起来） ，这一点使得 Batch Size 决不可太小，另外 Triplet Loss 的 BatchAll 方法也是需要一个 Batch 内有着特定选取的图片才会收敛（离线方式根本不收敛），所以我觉得我的 Batch Size 在本模型上还是有着一定的影响的

# References {docsify-ignore}

1. 刘雪虎、张国文学长答辩交流内容以及各位老师同学在本人答辩过程给出的建议
